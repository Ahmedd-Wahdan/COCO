{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f067ce0f",
   "metadata": {},
   "source": [
    "# BRIEF\n",
    "IT NOW CAN PERFORM REGRESSION AND multi class CLASSIFICATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load Fashion MNIST dataset using Scikit-Learn\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "images, labels = fashion_mnist[\"data\"], fashion_mnist[\"target\"].astype(int)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "images = images.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder()\n",
    "labels = encoder.fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Reshape images from (num_samples, 784) to (num_samples, 28, 28, 1)\n",
    "images = images.reshape((-1, 28 * 28))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_images, test_images = images[:60000], images[60000:]\n",
    "train_labels, test_labels = labels[:60000], labels[60000:]\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 128) \n",
    "        self.tanh = nn.Tanh() # First hidden layer (784 -> 128)\n",
    "        self.layer2 = nn.Linear(128, 64)    # Second hidden layer (128 -> 64)\n",
    "        self.layer3 = nn.Linear(64, 10)        # Output layer (64 -> 10)                 # ReLU activation function\n",
    "        self.softmax = nn.Softmax(dim=1)       # Softmax for output probabilities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return self.softmax(x)  # Softmax output\n",
    "\n",
    "# Instantiate the model\n",
    "model = FashionMNISTModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss combines Softmax and CrossEntropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(batch_y, 1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Testing the model\n",
    "def test(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(batch_y, 1)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Train and test the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=10)\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load Fashion MNIST dataset using Scikit-Learn\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "images, labels = fashion_mnist[\"data\"], fashion_mnist[\"target\"].astype(int)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "images = images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape images from (num_samples, 784) to (num_samples, 28, 28, 1)\n",
    "images = images.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_images, test_images = images[:60000], images[60000:]\n",
    "train_labels, test_labels = labels[:60000], labels[60000:]\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder()  # No need for sparse=False\n",
    "train_labels = encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "test_labels = encoder.transform(test_labels.reshape(-1, 1))\n",
    "\n",
    "# Flatten the images to (num_samples, 28 * 28)\n",
    "train_images = train_images.reshape((-1, 28 * 28))\n",
    "test_images = test_images.reshape((-1, 28 * 28))\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Train images shape:\", train_images.shape)  # (60000, 784)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # (60000, 10)\n",
    "print(\"Test images shape:\", test_images.shape)    # (10000, 784)\n",
    "print(\"Test labels shape:\", test_labels.shape)    # (10000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66146b8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e237b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 1, 28, 28)\n",
      "Train labels shape: (60000, 10)\n",
      "Test images shape: (10000, 1, 28, 28)\n",
      "Test labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load Fashion MNIST dataset using Scikit-Learn\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "images, labels = fashion_mnist[\"data\"], fashion_mnist[\"target\"].astype(int)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "images = images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape images from (num_samples, 784) to (num_samples, 1, 28, 28)\n",
    "images = images.reshape((-1, 1, 28, 28))  # Convert to 4D for Conv2D: [B, C, H, W]\n",
    "\n",
    "# Split into train and test sets\n",
    "train_images, test_images = images[:60000], images[60000:]\n",
    "train_labels, test_labels = labels[:60000], labels[60000:]\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(train_labels.reshape(-1, 1)).toarray()\n",
    "test_labels = encoder.transform(test_labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Train images shape:\", train_images.shape)  # (60000, 1, 28, 28)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # (60000, 10)\n",
    "print(\"Test images shape:\", test_images.shape)    # (10000, 1, 28, 28)\n",
    "print(\"Test labels shape:\", test_labels.shape)    # (10000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e16287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights (784, 10)\n",
      "bias (1, 10)\n",
      "Epoch 10/10 | [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||] 100.00% | Train Loss = 0.3965\n",
      "Function train took 1674.2703 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "from core.nn import Linear, Network,batchnorm1d,Conv2d,MaxPool2d\n",
    "from core.Function import tanh,relu\n",
    "conv1 = Conv2d(input_channels=1, output_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "max1 = MaxPool2d(kernel_size=2, stride=2)\n",
    "relu1 = relu()\n",
    "conv2 = Conv2d(input_channels=8, output_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "relu2 = relu()\n",
    "max2 = MaxPool2d(kernel_size=2, stride=2)\n",
    "linear1 = Linear((16 * 7 * 7,10),activation=\"softmax\")\n",
    "\n",
    "\n",
    "layers = [conv1,relu1,max1,conv2,relu2,max2,linear1]\n",
    "\n",
    "\n",
    "model_coco = Network(layers,classification=True)\n",
    "model_coco.train(train_images, train_labels, epochs=10, batch_size=64, learning_rate=0.001, optimizer=\"adam\",verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb97b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Network' object has no attribute 'featuremaps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_coco\u001b[38;5;241m.\u001b[39mfeaturemaps\u001b[38;5;241m.\u001b[39mshape()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Network' object has no attribute 'featuremaps'"
     ]
    }
   ],
   "source": [
    "model_coco.featuremaps.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f236ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_coco.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd36d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ypred_shape (10000, 1)\n",
      "y_true_shape (10000, 1)\n",
      "Accuracy on test set: 0.8525\n"
     ]
    }
   ],
   "source": [
    "# Reshape test data for Conv2d input (example for MNIST)\n",
    "test_images_4d = test_images.reshape(-1, 1, 28, 28)  # Adjust dimensions as needed\n",
    "\n",
    "# Predict\n",
    "y_pred = model_coco.predict(test_images_4d)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1, keepdims=True)\n",
    "\n",
    "# Use integer labels directly (if test_labels are not one-hot)\n",
    "y_true_classes = np.argmax(test_labels, axis=1, keepdims=True)  # Assuming test_labels are integers\n",
    "print(\"ypred_shape\",y_pred_classes.shape)\n",
    "print(\"y_true_shape\",y_true_classes.shape)\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred_classes == y_true_classes)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1cb504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3024\n",
      "Epoch 2/10, Loss: 2.3019\n",
      "Epoch 3/10, Loss: 2.3013\n",
      "Epoch 4/10, Loss: 2.3007\n",
      "Epoch 5/10, Loss: 2.2999\n",
      "Epoch 6/10, Loss: 2.2991\n",
      "Epoch 7/10, Loss: 2.2981\n",
      "Epoch 8/10, Loss: 2.2968\n",
      "Epoch 9/10, Loss: 2.2952\n",
      "Epoch 10/10, Loss: 2.2931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "# Load and preprocess the Fashion-MNIST dataset using the provided code\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "images, labels = fashion_mnist[\"data\"], fashion_mnist[\"target\"].astype(int)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "images = images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape images from (num_samples, 784) to (num_samples, 1, 28, 28)\n",
    "images = images.reshape((-1, 1, 28, 28))  # Convert to 4D for Conv2D: [B, C, H, W]\n",
    "\n",
    "# Split into train and test sets\n",
    "train_images, test_images = images[:60000], images[60000:]\n",
    "train_labels, test_labels = labels[:60000], labels[60000:]\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(train_labels.reshape(-1, 1)).toarray()\n",
    "test_labels = encoder.transform(test_labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the PyTorch model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 10)  # Fully connected layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = FashionMNISTModel()\n",
    "criterion = nn.CrossEntropyLoss()  # Note: Use raw class labels for CrossEntropyLoss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert one-hot encoded labels back to class indices for loss calculation\n",
    "train_labels_indices = torch.tensor(np.argmax(train_labels, axis=1), dtype=torch.long)\n",
    "test_labels_indices = torch.tensor(np.argmax(test_labels, axis=1), dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, epochs, optimizer, criterion):\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            labels = torch.argmax(labels, dim=1)  # Convert one-hot to class indices\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, epochs=10, optimizer=optimizer, criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ecbfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([64])\n",
      "labels shape torch.Size([64])\n",
      "predicted shape torch.Size([16])\n",
      "labels shape torch.Size([16])\n",
      "Accuracy on test data: 42.49%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, labels in test_loader:\n",
    "            labels = torch.argmax(labels, dim=1)  # Convert one-hot to class indices\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "            #print shapes\n",
    "            total += labels.size(0)  # Increment total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test data: {accuracy:.2f}%')\n",
    "\n",
    "# Test the model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4dee22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
